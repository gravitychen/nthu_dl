{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import ast\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "import numpy as np\n",
    "\n",
    "en_corpus = np.load('./dataset/translate/enCorpus.npy')\n",
    "en_vocab = np.load('./dataset/translate/enVocab.npy').tolist() # use tolist() to transform back to dict()\n",
    "en_rev = np.load('./dataset/translate/enRev.npy').tolist()\n",
    "\n",
    "# load conversations\n",
    "convs = open('dataset/chatbot/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "convs = [ str(i.replace(\"+++$+++ \",'')) for i in convs]\n",
    "convlist = [ re.sub('.+m[0-9]+ ','',i)  for i in convs]\n",
    "convlist = [ ast.literal_eval(i) for i in convlist[:-1]]\n",
    "\n",
    "talk=[]\n",
    "for j in convlist:\n",
    "    for i  in range(len(j)-1):\n",
    "        talk.append( j[i:i+2] )\n",
    "        \n",
    "for i in talk:\n",
    "    if len(i)!=2:\n",
    "        print(\"fuck\")\n",
    "# =============================split talk =================\n",
    "        \n",
    "        \n",
    "lines = open('dataset/chatbot/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n') \n",
    "lines = [ str(i.replace(\"+++$+++ \",'')) for i in lines]\n",
    "lines = [ re.sub(' u[0-9]+ m[0-9]+ [A-Z]+','',i).split(\" \",1)  for i in lines]   \n",
    "idict={}\n",
    "for i in lines[:-1]:\n",
    "    idict[i[0]]=i[1]\n",
    "    \n",
    "#===========================id sentence dict ===============\n",
    "\n",
    "realtalk=[]\n",
    "for i in talk:\n",
    "    try:\n",
    "        realtalk.append( [idict[i[0]],idict[i[1]]])\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator1:\n",
    "    def __init__(self, dat, batch_size):\n",
    "\n",
    "        self.batch_xs, self.batch_ys, self.reviews = [], [], []\n",
    "\n",
    "        self.batch_x ,self.batch_y=[],[]\n",
    "        \n",
    "        for i in realtalk:\n",
    "            if batch_size != 1:\n",
    "                self.batch_x.append( i[0].lower())\n",
    "                self.batch_y.append( i[1].lower())\n",
    "            elif batch_size==1:\n",
    "                self.batch_x=i[0].lower()\n",
    "                self.batch_y=i[1].lower()\n",
    "            \n",
    "        for j in range(0,len(realtalk),batch_size):\n",
    "            if batch_size != 1:\n",
    "                self.batch_xs.append(self.batch_x[j:j+batch_size])\n",
    "                self.batch_ys.append(self.batch_y[j:j+batch_size])\n",
    "            elif batch_size==1:\n",
    "                self.batch_xs.append( realtalk[j][0])\n",
    "                self.batch_ys.append( realtalk[j][1])\n",
    "            \n",
    "    def get(self):\n",
    "        return self.batch_xs, self.batch_ys\n",
    "    \n",
    "\n",
    "bg= BatchGenerator1(realtalk,1)\n",
    "x,y = bg.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x=[i.lower() for i in x]\n",
    "y=[i.lower() for i in y]\n",
    "x = [re.sub(r'[?|$|.|!|,|-|\"]|-+','',i) for i in x]\n",
    "y = [re.sub(r'[?|$|.|!|,|-|\"]|-+','',i) for i in y]\n",
    "import copy\n",
    "xx=copy.copy(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "z=x+y\n",
    "tid=[]\n",
    "for i in (z):\n",
    "    for j in re.findall(r'\\S+', i):\n",
    "        tid.append(j)\n",
    "A=Counter(tid)\n",
    "mydict = {x : A[x] for x in A if A[x] >= 30 }\n",
    "\n",
    "\n",
    "A=1\n",
    "for k,v in mydict.items():\n",
    "    mydict[k]=A\n",
    "    A+=1    \n",
    "\n",
    "\n",
    "mydict[\"<PAD>\"]=0\n",
    "revdict = {v: k for k, v in mydict.items()}\n",
    "\n",
    "\n",
    "for i in range(len(xx)-1,-1,-1):\n",
    "    #print(i)\n",
    "    state=0\n",
    "    for j in re.findall(r'\\S+', xx[i]):\n",
    "        try:\n",
    "            tid.append(mydict[j.lower()])\n",
    "        except:\n",
    "            state=1\n",
    "    if state==1:\n",
    "        del x[i];del y[i]     \n",
    "\n",
    "    \n",
    "yy=copy.copy(y)\n",
    "for i in range(len(yy)-1,-1,-1):\n",
    "    #print(i)\n",
    "    state=0\n",
    "    for j in re.findall(r'\\S+', yy[i]):\n",
    "        try:\n",
    "            tid.append(mydict[j.lower()])\n",
    "        except:\n",
    "            state=1\n",
    "    if state==1:\n",
    "        del x[i];del y[i]     \n",
    "\n",
    "    \n",
    "xid , yid= [],[]\n",
    "for i in range(len(x)):\n",
    "    tid=[]\n",
    "    for j in re.findall(r'\\S+', x[i]):\n",
    "        tid.append(mydict[j.lower()])\n",
    "    xid.append(tid)\n",
    "for i in range(len(y)):\n",
    "    tid=[]\n",
    "    for j in re.findall(r'\\S+', y[i]):\n",
    "        tid.append(mydict[j.lower()])\n",
    "    yid.append(tid)\n",
    "    \n",
    "\n",
    "\n",
    "q_max_len = 12\n",
    "a_max_len = 12\n",
    "#for i in range(len(xid)): # caculate max length\n",
    "#    q_max_len = max(q_max_len, len(xid[i]))\n",
    "#    a_max_len = max(a_max_len, len(yid[i]))\n",
    "#print(q_max_len, a_max_len)    \n",
    "for i in range(len(xid)-1,-1,-1):\n",
    "    if xid[i]==[] or yid[i]==[]:\n",
    "        del xid[i];del yid[i]\n",
    "    if len(xid[i])>q_max_len or len(yid[i])>q_max_len:\n",
    "        del xid[i];del yid[i]\n",
    "\n",
    "\n",
    "dbgx = []\n",
    "for i in xid:\n",
    "    tid=[]\n",
    "    for j in i:\n",
    "        tid.append(revdict[j])\n",
    "    dbgx.append(tid)\n",
    "    \n",
    "dbgy = []\n",
    "for i in yid:\n",
    "    tid=[]\n",
    "    for j in i:\n",
    "        tid.append(revdict[j])\n",
    "    dbgy.append(tid)\n",
    "\n",
    "\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, en_corpus, ch_corpus, en_pad, ch_pad, en_max_len, ch_max_len, batch_size):\n",
    "        assert len(en_corpus) == len(ch_corpus)\n",
    "        \n",
    "        batch_num = len(en_corpus)//batch_size\n",
    "        n = batch_num*batch_size\n",
    "        \n",
    "        self.xs = [np.zeros(n, dtype=np.int32) for _ in range(en_max_len)] # encoder inputs\n",
    "        self.ys = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder inputs\n",
    "        self.gs = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder outputs\n",
    "        self.ws = [np.zeros(n, dtype=np.float32) for _ in range(ch_max_len)] # decoder weight for loss caculation\n",
    "        \n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for b in range(batch_num):\n",
    "            for i in range(b*batch_size, (b+1)*batch_size):\n",
    "                for j in range(len(en_corpus[i])-2):\n",
    "                    self.xs[j][i] = en_corpus[i][j+1]\n",
    "                for j in range(j+1, en_max_len):\n",
    "                    self.xs[j][i] = en_pad\n",
    "                \n",
    "                for j in range(len(ch_corpus[i])-1):\n",
    "                    self.ys[j][i] = ch_corpus[i][j]\n",
    "                    self.gs[j][i] = ch_corpus[i][j+1]\n",
    "                    self.ws[j][i] = 1.0\n",
    "                for j in range(j+1, ch_max_len): # don't forget padding and let loss weight zero\n",
    "                    self.ys[j][i] = ch_pad\n",
    "                    self.gs[j][i] = ch_pad\n",
    "                    self.ws[j][i] = 0.0\n",
    "    \n",
    "    def get(self, batch_id):\n",
    "        x = [self.xs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.en_max_len)]\n",
    "        y = [self.ys[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        g = [self.gs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        w = [self.ws[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        \n",
    "        return x, y, g, w\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have my word as a <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "you're <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "sweet <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "looks like things worked out tonight <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "like things worked out tonight huh <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "fun <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "then that's all you had to <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "that's all you had to say <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch = BatchGenerator(xid, yid, \n",
    "                       mydict['<PAD>'], mydict['<PAD>'], q_max_len, a_max_len, 4)\n",
    "\n",
    "\n",
    "\n",
    "x, y, g, w = batch.get(1)\n",
    "for i in range(4):\n",
    "    print(' '.join([revdict[x[j][i]] for j in range(q_max_len)]))\n",
    "    print(' '.join([revdict[y[j][i]] for j in range(a_max_len)]))\n",
    "    print(' '.join([revdict[g[j][i]] for j in range(a_max_len)]))\n",
    "    print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class MachineTranslationSeq2Seq:\n",
    "    def __init__(self, en_max_len, ch_max_len, en_size, ch_size):\n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_intput/output'):\n",
    "            self.enc_inputs = [tf.placeholder(tf.int32, [None]) for i in range(en_max_len)] # time mojor feed\n",
    "            self.dec_inputs = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.groundtruths = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.weights = [tf.placeholder(tf.float32, [None]) for i in range(ch_max_len)]\n",
    "            \n",
    "        with tf.variable_scope('seq2seq_rnn'): # training by teacher forcing\n",
    "            self.out_cell = tf.contrib.rnn.LSTMCell(512)\n",
    "            self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                    self.out_cell, \n",
    "                                                                                    en_size, ch_size, 300)\n",
    "        with tf.variable_scope('seq2seq_rnn', reuse=True): # predict by feeding previous\n",
    "            self.pred_cell = tf.contrib.rnn.LSTMCell(512, reuse=True) # reuse cell for train and test\n",
    "            self.predictions, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                        self.pred_cell, \n",
    "                                                                                        en_size, ch_size, 300, \n",
    "                                                                                        feed_previous=True)\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            # caculate weighted loss\n",
    "            self.loss = tf.reduce_mean(tf.contrib.legacy_seq2seq.sequence_loss_by_example(self.outputs, \n",
    "                                                                                          self.groundtruths, \n",
    "                                                                                          self.weights))\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.002).minimize(self.loss)\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train(self, x, y, g, w):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i] # show how to feed a list\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "            fd[self.groundtruths[i]] = g[i]\n",
    "            fd[self.weights[i]] = w[i]\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss, self.optimizer], fd)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def output(self, x, y):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "        \n",
    "        out = self.sess.run(self.outputs, fd)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, x, ch_beg):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len): # when feed previous, the fist token should be '<BEG>', and others are useless\n",
    "            if i==0:\n",
    "                fd[self.dec_inputs[i]] = np.ones(y[i].shape, dtype=np.int32)*ch_beg\n",
    "            else:\n",
    "                fd[self.dec_inputs[i]] = np.zeros(y[i].shape, dtype=np.int32)\n",
    "        \n",
    "        pd = self.sess.run(self.predictions, fd)\n",
    "        \n",
    "        return pd\n",
    "    \n",
    "    def save(self, e):\n",
    "        self.saver.save(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e+1))\n",
    "    \n",
    "    def restore(self, e):\n",
    "        self.saver.restore(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = MachineTranslationSeq2Seq(q_max_len, a_max_len, len(mydict), len(mydict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = 256\n",
    "batch_num = len(xid)//BATCH_SIZE\n",
    "\n",
    "batch = BatchGenerator(xid, yid, \n",
    "                       mydict['<PAD>'], mydict['<PAD>'], \n",
    "                       q_max_len, a_max_len, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 5.916630\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of model/seq2seq/seq2seq_1.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: model/seq2seq; No such file or directory\n\t [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, loss/beta1_power, loss/beta2_power, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/embedding/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/embedding/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel/Adam_1, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/embedding, seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/embedding, seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias, seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1432\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: model/seq2seq; No such file or directory\n\t [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, loss/beta1_power, loss/beta2_power, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/embedding/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/embedding/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel/Adam_1, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/embedding, seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/embedding, seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias, seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-756a06403e43>\", line 2, in <module>\n    model = MachineTranslationSeq2Seq(q_max_len, a_max_len, len(mydict), len(mydict))\n  File \"<ipython-input-7-97d2ad3006b7>\", line 35, in __init__\n    self.saver = tf.train.Saver()\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1094, in __init__\n    self.build()\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1106, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1143, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 784, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 284, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 202, in save_op\n    tensors)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1690, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/Users/gravitychen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): model/seq2seq; No such file or directory\n\t [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, loss/beta1_power, loss/beta2_power, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/embedding/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/embedding/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/embedding/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias/Adam_1, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel/Adam, loss/seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel/Adam_1, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/bias, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/output_projection_wrapper/lstm_cell/kernel, seq2seq_rnn/embedding_attention_seq2seq/embedding_attention_decoder/embedding, seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/embedding, seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/bias, seq2seq_rnn/embedding_attention_seq2seq/rnn/embedding_wrapper/lstm_cell/kernel)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8073036e4e57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch %d loss: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/seq2seq/rec_loss.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-97d2ad3006b7>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model/seq2seq/seq2seq_%d.ckpt'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1448\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[1;32m   1449\u001b[0m                   save_path))\n\u001b[0;32m-> 1450\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of model/seq2seq/seq2seq_1.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "for e in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for b in range(batch_num):\n",
    "        x, y, g, w = batch.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "    \n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    print(\"epoch %d loss: %f\" % (e, train_loss))\n",
    "    \n",
    "    model.save(e)\n",
    "    \n",
    "np.save('./model/seq2seq/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
